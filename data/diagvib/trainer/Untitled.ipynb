{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d54255e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-00e21d2cad7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiagvibsix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTorchDatasetWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "#!/usr/local/bin/python3\n",
    "# Copyright (c) 2021 Robert Bosch GmbH Copyright holder of the paper \"DiagViB-6: A Diagnostic Benchmark Suite for Vision Models in the Presence of Shortcut and Generalization Opportunities\" accepted at ICCV 2021.\n",
    "# All rights reserved.\n",
    "###\n",
    "# The paper \"DiagViB-6: A Diagnostic Benchmark Suite for Vision Models in the Presence of Shortcut and Generalization Opportunities\" accepted at ICCV 2021.\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Affero General Public License as published\n",
    "# by the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n",
    "# GNU Affero General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU Affero General Public License\n",
    "# along with this program. If not, see <https://www.gnu.org/licenses/>.\n",
    "#\n",
    "# Author: Elias Eulig, Volker Fischer\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from diagvibsix import TorchDatasetWrapper\n",
    "from diagvibsix.auxiliaries import save_obj, save_yaml\n",
    "from utils.metrics import Losses, Metrics\n",
    "\n",
    "__all__ = [\n",
    "    'setup_optimizer',\n",
    "    'setup_loss',\n",
    "    'setup_dataloader',\n",
    "    'BaseTrainer',\n",
    "]\n",
    "\n",
    "\n",
    "def setup_optimizer(args, parameters):\n",
    "    \"\"\"Setup the optimizer.\n",
    "\n",
    "    Args:\n",
    "        args (argparser.Namespace): Namespace containing optimizer settings.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: Class instance for the respective optimizer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if args.optimizer == 'sgd':\n",
    "        return optim.SGD(parameters,\n",
    "                         lr=args.lr,\n",
    "                         momentum=args.sgd_momentum,\n",
    "                         dampening=args.sgd_dampening\n",
    "                         )\n",
    "    elif args.optimizer == 'adam':\n",
    "        return optim.Adam(parameters,\n",
    "                          lr=args.lr,\n",
    "                          betas=(args.adam_b1, args.adam_b2))\n",
    "    elif args.optimizer == 'rmsprop':\n",
    "        return optim.RMSprop(parameters,\n",
    "                             lr=args.lr)\n",
    "    else:\n",
    "        raise ValueError('Optimizer unknown. Must be sgd | adam | rmsprop')\n",
    "\n",
    "\n",
    "def setup_loss(loss, **kwargs):\n",
    "    \"\"\"Setup the loss.\n",
    "\n",
    "    Args:\n",
    "        loss (str): Loss function to use\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: Class instance for the respective loss.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if loss == 'ce':\n",
    "        return nn.CrossEntropyLoss()\n",
    "    elif loss == 'bce':\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "    elif loss == 'mse':\n",
    "        return nn.MSELoss(reduction='mean')\n",
    "    else:\n",
    "        raise ValueError('Loss function unknown.')\n",
    "\n",
    "\n",
    "def setup_dataloader(args, datasets):\n",
    "    \"\"\"Given the main args and a dict of datasets returns respective dataloaders.\n",
    "\n",
    "    Args:\n",
    "        loss (str): Loss function to use\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: Class instance for the respective loss.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    data_loader = {phase: DataLoader(dataset=data, batch_size=args.mbs, num_workers=args.num_workers,\n",
    "                                     pin_memory=args.device >= 0, shuffle=True, drop_last=True)\n",
    "                   for phase, data in datasets.items()}\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "class BaseTrainer(object):\n",
    "    \"\"\"Base class to benchmark a model.\n",
    "\n",
    "    This is the base class to benchmark a model on DiagViB-6. All model-specific\n",
    "    trainers should be inheretid from this base class and then overwrite methods\n",
    "    if needed (see trainer/trainer_setup.py for an example).\n",
    "\n",
    "    Attributes:\n",
    "        args (argparser.Namespace): Namespace containing training arguments\n",
    "        dev (torch.device): Device to perform training on\n",
    "        benchmark_path (string): Benchmark path for this study/experiment/sample\n",
    "        results_path (string): Path to store results to\n",
    "        specs (dict): Dict storing the paths to dataset specifications\n",
    "        data (dict): Dict storing the datasets\n",
    "        data_loader (dict): Dict storing the data loaders\n",
    "        class_criterion (torch.nn.module): Torch loss module\n",
    "        tags (dict): Dict containing the tags for each phase\n",
    "        task (str): String of the task (e.g. shape)\n",
    "        shape (tuple): Tuple of input image shape\n",
    "        losses (Losses): Losses object tracking training and validation losses\n",
    "        metrics (Metrics): Metrics object tracking train/val/test metrics\n",
    "        model (None): Placeholder for the actual model\n",
    "        optimizer (None): Placeholder for the actual optimizer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.dev = torch.device(\n",
    "            'cuda', args.device) if args.device >= 0 else torch.device('cpu')\n",
    "\n",
    "        self.benchmark_path = os.path.join(args.study, args.experiment,\n",
    "                                           args.dataset_sample)\n",
    "        self.results_path = os.path.join(self.args.results_path,\n",
    "                                         self.benchmark_path, self.args.method)\n",
    "\n",
    "        # Generate paths to dataset specifications.\n",
    "        study_path = os.path.join(args.study_folder, self.benchmark_path)\n",
    "        self.specs = {t: os.path.join(study_path, t + '.yml')\n",
    "                      for t in ['train', 'val', 'test']}\n",
    "\n",
    "        self.data = {'train': TorchDatasetWrapper(self.specs['train'],\n",
    "                                                  args.dataset_seed,\n",
    "                                                  cache=args.cache)}\n",
    "        self.data = {self.data, {phase: TorchDatasetWrapper(self.specs[phase], args.dataset_seed + i + 1,\n",
    "                                                                mean=self.data['train'].mean,\n",
    "                                                                std=self.data['train'].std,\n",
    "                                                                cache=args.cache)\n",
    "                                     for i, phase in enumerate(['val', 'test'])}}\n",
    "\n",
    "        # Setup data loader.\n",
    "        self.data_loader = setup_dataloader(args, self.data)\n",
    "\n",
    "        # Setup criterion.\n",
    "        self.class_criterion = setup_loss(args.class_criterion)\n",
    "\n",
    "        # Get train, val and test tags and task\n",
    "        self.tags = {\n",
    "            phase: self.data[phase].tags for phase in self.specs.keys()}\n",
    "        self.task = self.data['train'].task\n",
    "\n",
    "        # Get shape\n",
    "        self.shape = self.data['train'].shape\n",
    "\n",
    "        # Setup logging information\n",
    "        self.losses = Losses(losses_to_log=['train', 'val'],\n",
    "                             data_loader=self.data_loader)\n",
    "\n",
    "        metrics_to_log = {'train': ['per_class_accuracy', 'cm'],\n",
    "                          'val': ['per_class_accuracy', 'cm'],\n",
    "                          'test': ['per_class_accuracy']}\n",
    "        self.metrics = Metrics(metrics_to_log, self.task, self.tags)\n",
    "\n",
    "        # Setup placeholders for model and optimizer\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "\n",
    "        self.epoch = 0\n",
    "\n",
    "    def train_step(self, batch, tags):\n",
    "        input, target = batch\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        out = self.model(input)\n",
    "\n",
    "        loss = self.class_criterion(out, target)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.losses.push(loss.item(), 'train')\n",
    "        self.metrics.push(out, target, tags, 'train')\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        for i_batch, batch in enumerate(tqdm(self.data_loader['train'], desc='Train')):\n",
    "            tags = batch.pop('tag')\n",
    "            batch = (Variable(v).to(self.dev, non_blocking=True)\n",
    "                     for v in batch.values())\n",
    "            self.train_step(batch, tags)\n",
    "\n",
    "        self.losses.summarize('train')\n",
    "        self.metrics.summarize('train')\n",
    "\n",
    "    def val_step(self, i_batch, batch, tags):\n",
    "        input, target = batch\n",
    "\n",
    "        out = self.model(input)\n",
    "\n",
    "        loss = self.class_criterion(out, target)\n",
    "\n",
    "        self.losses.push(loss.item(), 'val')\n",
    "        self.metrics.push(out, target, tags, 'val')\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i_batch, batch in enumerate(tqdm(self.data_loader['val'], desc='Validate')):\n",
    "                tags = batch.pop('tag')\n",
    "                batch = (v.to(self.dev, non_blocking=True)\n",
    "                         for v in batch.values())\n",
    "                self.val_step(i_batch, batch, tags)\n",
    "\n",
    "        self.losses.summarize('val')\n",
    "        self.metrics.summarize('val')\n",
    "\n",
    "    def test_step(self, batch, tags):\n",
    "        input, target = batch\n",
    "        out = self.model(input)\n",
    "        self.metrics.push(out, target, tags, 'test')\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i_batch, batch in enumerate(tqdm(self.data_loader['test'], desc='Test')):\n",
    "                tags = batch.pop('tag')\n",
    "                batch = (v.to(self.dev, non_blocking=True)\n",
    "                         for v in batch.values())\n",
    "                self.test_step(batch, tags)\n",
    "\n",
    "        self.metrics.summarize('test')\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        save_path = os.path.join(self.results_path, 'checkpoints')\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        for phase in ['val']:\n",
    "            if self.epoch == np.argmin(self.losses.losses[phase]):\n",
    "                checkpoint_path = os.path.join(\n",
    "                    save_path, 'best_{}.pt'.format(phase))\n",
    "                torch.save({'epoch': self.epoch,\n",
    "                            'model_state_dict': self.model.state_dict()\n",
    "                            }, checkpoint_path\n",
    "                           )\n",
    "\n",
    "    def load_checkpoint(self, path, only_weights=False):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        if not only_weights:\n",
    "            self.epoch = checkpoint['epoch']\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    def log_stats(self):\n",
    "        # Log losses and metrics\n",
    "        plts_save_path = os.path.join(self.results_path, 'plts')\n",
    "        if not os.path.exists(plts_save_path):\n",
    "            os.makedirs(plts_save_path)\n",
    "        self.metrics.log(phases=['train', 'val'], save_path=plts_save_path)\n",
    "        self.losses.log(save_path=plts_save_path)\n",
    "\n",
    "    def fit(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        for epoch in trange(self.epoch, self.args.num_epochs):\n",
    "            torch.manual_seed(self.args.training_seed + epoch)\n",
    "            # Train val test loop\n",
    "            self.train()\n",
    "            self.validate()\n",
    "\n",
    "            # Logging\n",
    "            self.log_stats()\n",
    "            self.save_checkpoint()\n",
    "\n",
    "            # Losses and metrics reset\n",
    "            self.metrics.reset()\n",
    "            self.losses.reset()\n",
    "            self.epoch += 1\n",
    "\n",
    "        time_elapsed = time.time() - self.start_time\n",
    "        print('Training completed in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    def run(self):\n",
    "        # Create results directory and save args + start command\n",
    "        if not os.path.exists(self.results_path):\n",
    "            os.makedirs(self.results_path)\n",
    "        save_yaml(vars(self.args), os.path.join(self.results_path, 'args.yaml'))\n",
    "\n",
    "        # Run training.\n",
    "        self.fit()\n",
    "\n",
    "        # Load best validation net and run test\n",
    "        print('Run test using best validation network ...')\n",
    "        checkpoint_path = os.path.join(self.results_path, 'checkpoints',\n",
    "                                       'best_val.pt')\n",
    "        self.load_checkpoint(checkpoint_path, only_weights=True)\n",
    "        self.test()\n",
    "        self.metrics.log(phases='test',\n",
    "                         save_path=os.path.join(self.results_path, 'plts'))\n",
    "\n",
    "        # Save losses and metric histories to pkl file\n",
    "        save_obj({'metrics': self.metrics.metrics, 'losses': self.losses.losses},\n",
    "                 os.path.join(self.results_path, 'stats.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89efaec1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TorchDatasetWrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6811a772a4f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m self.data = {'train': TorchDatasetWrapper(self.specs['train'],\n\u001b[0m\u001b[1;32m      2\u001b[0m                                           \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                           cache=args.cache)}\n\u001b[1;32m      4\u001b[0m self.data = {self.data, {phase: TorchDatasetWrapper(self.specs[phase], args.dataset_seed + i + 1,\n\u001b[1;32m      5\u001b[0m                                                         \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TorchDatasetWrapper' is not defined"
     ]
    }
   ],
   "source": [
    "self.data = {'train': TorchDatasetWrapper(self.specs['train'],\n",
    "                                          args.dataset_seed,\n",
    "                                          cache=args.cache)}\n",
    "self.data = {self.data, {phase: TorchDatasetWrapper(self.specs[phase], args.dataset_seed + i + 1,\n",
    "                                                        mean=self.data['train'].mean,\n",
    "                                                        std=self.data['train'].std,\n",
    "                                                        cache=args.cache)\n",
    "                             for i, phase in enumerate(['val', 'test'])}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd229c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
